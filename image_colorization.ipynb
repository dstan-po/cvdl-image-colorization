{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-o_C5kVEITI"
      },
      "outputs": [],
      "source": [
        "!pip install fastai == 2.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.data.external import untar_data, URLs\n",
        "\n",
        "DATASET_PATH = untar_data(URLs.COCO_SAMPLE)\n",
        "DATASET_PATH = str(DATASET_PATH) + \"/train_sample\""
      ],
      "metadata": {
        "id": "CP7y0oXjFxXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atlw63EEED1x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "\n",
        "from fastai.vision.learner import create_body\n",
        "from torchvision.models.resnet import resnet18\n",
        "from fastai.vision.models.unet import DynamicUnet\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "PATH = DATASET_PATH\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Metrics:\n",
        "\tdef __init__(self):\n",
        "\t\tself.reset()\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tself.count, self.avg, self.sum = [0.] * 3\n",
        "\n",
        "\tdef update(self, val, count=1):\n",
        "\t\tself.count += count\n",
        "\t\tself.sum += count * val\n",
        "\t\tself.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def create_loss_meters():\n",
        "\tloss_discriminator_fake = Metrics()\n",
        "\tloss_discriminator_real = Metrics()\n",
        "\tloss_discriminator = Metrics()\n",
        "\tloss_generator_GAN = Metrics()\n",
        "\tloss_generator_L1 = Metrics()\n",
        "\tloss_generator = Metrics()\n",
        "\n",
        "\treturn {'loss_discriminator_fake': loss_discriminator_fake,\n",
        "\t\t\t'loss_discriminator_real': loss_discriminator_real,\n",
        "\t\t\t'loss_discriminator': loss_discriminator,\n",
        "\t\t\t'loss_generator_GAN': loss_generator_GAN,\n",
        "\t\t\t'loss_generator_L1': loss_generator_L1,\n",
        "\t\t\t'loss_generator': loss_generator}\n",
        "\n",
        "\n",
        "def update_losses(model, loss_meter_dict, count):\n",
        "\tfor loss_name, loss_meter in loss_meter_dict.items():\n",
        "\t\tloss = getattr(model, loss_name)\n",
        "\t\tloss_meter.update(loss.item(), count=count)\n",
        "\n",
        "LIGHTNESS = 'lightness'\n",
        "AB_CHANNELS = 'ab_channels'\n",
        "\n",
        "def lab_to_rgb(L, ab):\n",
        "\t\"\"\"\n",
        "    Takes a batch of images\n",
        "    \"\"\"\n",
        "\n",
        "\tL = (L + 1.) * 50.\n",
        "\tab = ab * 110.\n",
        "\tLab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "\trgb_imgs = []\n",
        "\tfor img in Lab:\n",
        "\t\timg_rgb = lab2rgb(img)\n",
        "\t\trgb_imgs.append(img_rgb)\n",
        "\treturn np.stack(rgb_imgs, axis=0)\n",
        "\n",
        "\n",
        "def show_results(model, data, save=True):\n",
        "\tmodel.net_G.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tmodel.setup_input(data)\n",
        "\t\tmodel.forward()\n",
        "\tmodel.net_G.train()\n",
        "\tfake_color = model.fake_color.detach()\n",
        "\treal_color = model.ab_channels\n",
        "\tL = model.lightness\n",
        "\tfake_imgs = lab_to_rgb(L, fake_color)\n",
        "\treal_imgs = lab_to_rgb(L, real_color)\n",
        "\tfig = plt.figure(figsize=(16, 8))\n",
        "\tfor i in range(5):\n",
        "\t\tax = plt.subplot(3, 5, i + 1)\n",
        "\t\tax.imshow(L[i][0].cpu(), cmap='gray')\n",
        "\t\tax.axis(\"off\")\n",
        "\t\tax = plt.subplot(3, 5, i + 1 + 5)\n",
        "\t\tax.imshow(fake_imgs[i])\n",
        "\t\tax.axis(\"off\")\n",
        "\t\tax = plt.subplot(3, 5, i + 1 + 10)\n",
        "\t\tax.imshow(real_imgs[i])\n",
        "\t\tax.axis(\"off\")\n",
        "\tplt.show()\n",
        "\tif save:\n",
        "\t\tfig.savefig(f\"colorization_{time.time()}.png\")\n",
        "\n",
        "\n",
        "def log_results(loss_meter_dict):\n",
        "\tfor loss_name, loss_meter in loss_meter_dict.items():\n",
        "\t\tprint(f\"{loss_name}: {loss_meter.avg:.5f}\")\n",
        "\n",
        "\n",
        "SIZE = 256\n",
        "TRAIN_TRANSFORM = 'train'\n",
        "VALIDATION_TRANSFORM = 'validation'\n",
        "\n",
        "class ColorizationDataset(Dataset):\n",
        "\tdef __init__(self, paths, split=TRAIN_TRANSFORM):\n",
        "\t\tif split == TRAIN_TRANSFORM:\n",
        "\t\t\tself.transforms = transforms.Compose([transforms.Resize((SIZE, SIZE), Image.BICUBIC), transforms.RandomHorizontalFlip()])\n",
        "\t\telif split == VALIDATION_TRANSFORM:\n",
        "\t\t\tself.transforms = transforms.Resize((SIZE, SIZE), Image.BICUBIC)\n",
        "\n",
        "\t\tself.split = split\n",
        "\t\tself.size = SIZE\n",
        "\t\tself.paths = paths\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\timg = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "\t\timg = self.transforms(img)\n",
        "\t\timg = np.array(img)\n",
        "\t\timg_lab = rgb2lab(img).astype(\"float32\")\n",
        "\t\timg_lab = transforms.ToTensor()(img_lab)\n",
        "\t\tlightness = img_lab[[0], ...] / 50. - 1.\n",
        "\t\tab_channels = img_lab[[1, 2], ...] / 110.\n",
        "\n",
        "\t\treturn {LIGHTNESS: lightness, AB_CHANNELS: ab_channels}\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.paths)\n",
        "\n",
        "\n",
        "paths = glob.glob(PATH + \"/*.jpg\")\n",
        "paths_subset = np.random.choice(paths, 10000, replace=False)\n",
        "random_indexes = np.random.permutation(10000)\n",
        "\n",
        "train_paths = paths_subset[random_indexes[:8000]] # The first 8000 images\n",
        "\n",
        "val_paths = paths_subset[random_indexes[8000:]] # The remaining 2000 images\n",
        "print(len(train_paths), len(val_paths))\n",
        "\n",
        "DEFAULT_BATCH_SIZE = 16\n",
        "NUMBER_OF_WORKERS = 16\n",
        "\n",
        "def create_data_loader(batch_size=DEFAULT_BATCH_SIZE, n_workers=NUMBER_OF_WORKERS, pin_memory=True, **kwargs):\n",
        "\treturn DataLoader(ColorizationDataset(**kwargs), batch_size=batch_size, num_workers=n_workers, pin_memory=pin_memory)\n",
        "\n",
        "training_data_loader = create_data_loader(paths=train_paths, split=TRAIN_TRANSFORM)\n",
        "\n",
        "\n",
        "def build_res_unet(n_input=1, n_output=2, size=256):\n",
        "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\tbody = create_body(resnet18, pretrained=True, n_in=n_input, cut=-2)\n",
        "\treturn DynamicUnet(body, n_output, (size, size)).to(device)\n",
        "\n",
        "\n",
        "def pretrain_generator(net_G, train_dl, opt, criterion, epochs):\n",
        "\tfor e in range(epochs):\n",
        "\t\tloss_meter = Metrics()\n",
        "\n",
        "\t\tfor data in tqdm(train_dl): # Iteration with loading\n",
        "\t\t\tlightness, ab_channels = data[LIGHTNESS].to(DEVICE), data[AB_CHANNELS].to(DEVICE)\n",
        "\t\t\tpreds = net_G(lightness)\n",
        "\t\t\tloss = criterion(preds, ab_channels)\n",
        "\t\t\topt.zero_grad()\n",
        "\t\t\tloss.backward()\n",
        "\t\t\topt.step()\n",
        "\n",
        "\t\t\tloss_meter.update(loss.item(), lightness.size(0))\n",
        "\n",
        "\t\tprint(f\"Epoch {e + 1}/{epochs}\")\n",
        "\t\tprint(f\"L1 Loss: {loss_meter.avg:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training generator"
      ],
      "metadata": {
        "id": "mxbCWmOC4E_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
        "opt = optim.Adam(net_G.parameters(), lr=1e-4)\n",
        "criterion = nn.L1Loss()\n",
        "pretrain_generator(net_G, training_data_loader, opt, criterion, 20)\n",
        "# torch.save(net_G.state_dict(), \"res18-unet.pt\")"
      ],
      "metadata": {
        "id": "mO-e4XG2UQ3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MainModel(nn.Module):\n",
        "\tdef __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
        "\t\t\t\t beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\t\tself.lambda_L1 = lambda_L1\n",
        "\n",
        "\t\tif net_G is None:\n",
        "\t\t\tself.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
        "\t\telse:\n",
        "\t\t\tself.net_G = net_G.to(self.device)\n",
        "\t\tself.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
        "\t\tself.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
        "\t\tself.L1criterion = nn.L1Loss()\n",
        "\t\tself.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
        "\t\tself.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
        "\n",
        "\tdef set_requires_grad(self, model, requires_grad=True):\n",
        "\t\tfor p in model.parameters():\n",
        "\t\t\tp.requires_grad = requires_grad\n",
        "\n",
        "\tdef setup_input(self, data):\n",
        "\t\tself.lightness = data['L'].to(self.device)\n",
        "\t\tself.ab_channels = data['ab'].to(self.device)\n",
        "\n",
        "\tdef forward(self):\n",
        "\t\tself.fake_color = self.net_G(self.lightness)\n",
        "\n",
        "\tdef backward_D(self):\n",
        "\t\tfake_image = torch.cat([self.lightness, self.fake_color], dim=1)\n",
        "\t\tfake_preds = self.net_D(fake_image.detach())\n",
        "\t\tself.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
        "\t\treal_image = torch.cat([self.lightness, self.ab_channels], dim=1)\n",
        "\t\treal_preds = self.net_D(real_image)\n",
        "\t\tself.loss_D_real = self.GANcriterion(real_preds, True)\n",
        "\t\tself.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "\t\tself.loss_D.backward()\n",
        "\n",
        "\tdef backward_G(self):\n",
        "\t\tfake_image = torch.cat([self.lightness, self.fake_color], dim=1)\n",
        "\t\tfake_preds = self.net_D(fake_image)\n",
        "\t\tself.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
        "\t\tself.loss_G_L1 = self.L1criterion(self.fake_color, self.ab_channels) * self.lambda_L1\n",
        "\t\tself.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "\t\tself.loss_G.backward()\n",
        "\n",
        "\tdef optimize(self):\n",
        "\t\tself.forward()\n",
        "\t\tself.net_D.train()\n",
        "\t\tself.set_requires_grad(self.net_D, True)\n",
        "\t\tself.opt_D.zero_grad()\n",
        "\t\tself.backward_D()\n",
        "\t\tself.opt_D.step()\n",
        "\n",
        "\t\tself.net_G.train()\n",
        "\t\tself.set_requires_grad(self.net_D, False)\n",
        "\t\tself.opt_G.zero_grad()\n",
        "\t\tself.backward_G()\n",
        "\t\tself.opt_G.step()\n",
        "\n",
        "\n",
        "def init_weights(net, init='norm', gain=0.02):\n",
        "\tdef init_func(m):\n",
        "\t\tclassname = m.__class__.__name__\n",
        "\t\tif hasattr(m, 'weight') and 'Conv' in classname:\n",
        "\t\t\tif init == 'norm':\n",
        "\t\t\t\tnn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "\t\t\telif init == 'xavier':\n",
        "\t\t\t\tnn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "\t\t\telif init == 'kaiming':\n",
        "\t\t\t\tnn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "\n",
        "\t\t\tif hasattr(m, 'bias') and m.bias is not None:\n",
        "\t\t\t\tnn.init.constant_(m.bias.data, 0.0)\n",
        "\t\telif 'BatchNorm2d' in classname:\n",
        "\t\t\tnn.init.normal_(m.weight.data, 1., gain)\n",
        "\t\t\tnn.init.constant_(m.bias.data, 0.)\n",
        "\n",
        "\tnet.apply(init_func)\n",
        "\tprint(f\"model initialized with {init} initialization\")\n",
        "\treturn net\n",
        "\n",
        "\n",
        "def init_model(model, device):\n",
        "\tmodel = model.to(device)\n",
        "\tmodel = init_weights(model)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "\tdef __init__(self, input_c, num_filters=64, n_down=3):\n",
        "\t\tsuper().__init__()\n",
        "\t\tmodel = [self.get_layers(input_c, num_filters, norm=False)]\n",
        "\t\tmodel += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down - 1) else 2) for i in range(n_down)]\n",
        "\t\tmodel += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)]\n",
        "\t\tself.model = nn.Sequential(*model)\n",
        "\n",
        "\tdef get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True):\n",
        "\t\tlayers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]\n",
        "\t\tif norm:\n",
        "\t\t\tlayers += [nn.BatchNorm2d(nf)]\n",
        "\t\tif act:\n",
        "\t\t\tlayers += [nn.LeakyReLU(0.2, True)]\n",
        "\t\treturn nn.Sequential(*layers)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.model(x)\n",
        "\n",
        "\n",
        "class GANLoss(nn.Module):\n",
        "\tdef __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.register_buffer('real_label', torch.tensor(real_label))\n",
        "\t\tself.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "\t\tif gan_mode == 'vanilla':\n",
        "\t\t\tself.loss = nn.BCEWithLogitsLoss()\n",
        "\t\telif gan_mode == 'lsgan':\n",
        "\t\t\tself.loss = nn.MSELoss()\n",
        "\n",
        "\tdef get_labels(self, preds, target_is_real):\n",
        "\t\tif target_is_real:\n",
        "\t\t\tlabels = self.real_label\n",
        "\t\telse:\n",
        "\t\t\tlabels = self.fake_label\n",
        "\t\treturn labels.expand_as(preds)\n",
        "\n",
        "\tdef __call__(self, preds, target_is_real):\n",
        "\t\tlabels = self.get_labels(preds, target_is_real)\n",
        "\t\tloss = self.loss(preds, labels)\n",
        "\t\treturn loss"
      ],
      "metadata": {
        "id": "nrWgpp0KmNrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model"
      ],
      "metadata": {
        "id": "5lTk9xxnUfwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dl, epochs, display_every=200):\n",
        "\tdata = next(iter(val_dl))\n",
        "\tfor e in range(epochs):\n",
        "\t\tloss_meter_dict = create_loss_meters()\n",
        "\t\ti = 0\n",
        "\t\tfor data in tqdm(train_dl):\n",
        "\t\t\tmodel.setup_input(data)\n",
        "\t\t\tmodel.optimize()\n",
        "\t\t\tupdate_losses(model, loss_meter_dict, count=data['L'].size(0))\n",
        "\t\t\ti += 1\n",
        "\t\t\tif i % display_every == 0:\n",
        "\t\t\t\tprint(f\"\\nEpoch {e + 1}/{epochs}\")\n",
        "\t\t\t\tprint(f\"Iteration {i}/{len(train_dl)}\")\n",
        "\t\t\t\tlog_results(loss_meter_dict)\n",
        "\t\t\t\tshow_results(model, data, save=False)\n",
        "\n",
        "\n",
        "val_dl = create_data_loader(paths=val_paths, split=VALIDATION_TRANSFORM)"
      ],
      "metadata": {
        "id": "Kib1VTCwiLjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
        "net_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=DEVICE))\n",
        "model = MainModel(net_G=net_G)\n",
        "train_model(model, training_data_loader, 20)\n",
        "torch.save(model.state_dict(), \"final_model.pt\")"
      ],
      "metadata": {
        "id": "217fWhCQUhNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "zwkxeY0V0oUJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FINAL_PROJ (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}